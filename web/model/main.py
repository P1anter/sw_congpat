# main.py (sw/web/model/ ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜)

import os
import json
import torch
import pandas as pd
from torch.utils.data import DataLoader, Subset

# FastAPI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse

# --- 1. í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ (test_v5.pyì™€ ë™ì¼) ---
# (main.pyì™€ p_dataloader_5_3.pyê°€ ê°™ì€ í´ë”ì— ìˆìœ¼ë¯€ë¡œ ì˜¤ë¥˜ ì—†ì´ ì‘ë™)
from p_dataloader_5_3 import (
    HadmTableDatasetV3, collate_hadm_batch_v3, example_sources_config_v3
)
from architectures.predictor.predict_modelv2 import TableTransformerPredictor

# --- 2. ê²½ë¡œ ì„¤ì • (ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜) ---
# í˜„ì¬ íŒŒì¼(main.py)ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤. (sw/web/model/)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# test_v5.pyì˜ ìƒëŒ€ ê²½ë¡œë¥¼ BASE_DIR ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œí™”í•©ë‹ˆë‹¤.
PATIENTS_CSV = os.path.join(BASE_DIR, "./filtered/patients2.csv")
UNIFIED_CSV = os.path.join(BASE_DIR, "../dataset/summarized_with_readmit30_test.csv")
CKPT_PATH   = os.path.join(BASE_DIR, "./checkpoints_exp_final/highperf_best_exp_final.pt")
CACHE_DIR   = os.path.join(BASE_DIR, "./latent_cache_v2_exp_final")

USE_EXAMPLE_SOURCES = True
SOURCES_JSON_PATH   = None
BATCH_SIZE = 64

if torch.cuda.is_available():
    ENCODE_DEVICE = "cuda"
elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    ENCODE_DEVICE = "mps"
else:
    ENCODE_DEVICE = "cpu"

# --- ITEM IDì™€ í•œêµ­ì–´ ê²€ì‚¬ëª… ë§¤í•‘ ---
ITEMID_MAPPING = {
    50868: "íƒ„ì‚°ìˆ˜ì†Œ(Bicarb)",  # 50868ì€ CO2 (Bicarb)ì¼ í™•ë¥ ì´ ë†’ìŒ
    50882: "íƒ„ì‚°ìˆ˜ì†Œ(Bicarb)",  # 50882 ì—­ì‹œ CO2 (Bicarb)ì¼ í™•ë¥ ì´ ë†’ìŒ
    50893: "í´ë¡œë¼ì´ë“œ(Chloride)", # 50893ì€ Chlorideì¼ í™•ë¥ ì´ ë†’ìŒ
    50902: "ì¹¼ìŠ˜(Calcium)",    # 50902ëŠ” Calciumì¼ í™•ë¥ ì´ ë†’ìŒ
    50912: "í¬ë ˆì•„í‹°ë‹Œ(Creatinine)", # 50912ëŠ” Creatinineì¼ í™•ë¥ ì´ ë†’ìŒ
    50931: "í¬ë„ë‹¹(Glucose)",    # 50931ì€ Glucoseì¼ í™•ë¥ ì´ ë†’ìŒ
    50971: "ì¹¼ë¥¨(Potassium)", 
    50983: "ë‚˜íŠ¸ë¥¨(Sodium)",
    51006: "í˜ˆì¤‘ìš”ì†Œì§ˆì†Œ(BUN)",
    # í•„ìš”í•œ ë‹¤ë¥¸ itemidê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
}

# --- 3. í•µì‹¬ ë¡œì§ (test_v5.pyì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬) ---

# =========================
# Utilities
# =========================
def resolve_device(device=None):
    if device is not None:
        return torch.device(device)
    if torch.cuda.is_available():
        return torch.device("cuda")
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def _pick_col(df: pd.DataFrame, candidates):
    """ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  í›„ë³´ ì¤‘ ì¡´ì¬í•˜ëŠ” ì²« ì»¬ëŸ¼ëª…ì„ ë°˜í™˜"""
    lower = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lower:
            return lower[cand.lower()]
    return None

# ----------------------------------------------------------------------------
# 3. í•µì‹¬ ë¡œì§: ìƒˆ í•¨ìˆ˜ ì¶”ê°€
# ----------------------------------------------------------------------------

def lookup_lab_tests(subject_id_str: str, unified_csv_path: str):
    """íŠ¹ì • í™˜ìì˜ ê²€ì‚¬ ê¸°ë¡ì„ UNIFIED_CSVì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        # UNIFIED_CSV (summarized_with_readmit30_test.csv) íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤.
        df_full = pd.read_csv(unified_csv_path) 
    except FileNotFoundError:
        return "âŒ ì˜¤ë¥˜: í†µí•© ë°ì´í„° íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", None
    
    try:
        subject_id = int(subject_id_str)
    except ValueError:
        return "âŒ ì˜¤ë¥˜: subject_idê°€ ìœ íš¨í•˜ì§€ ì•Šì€ í˜•ì‹ì…ë‹ˆë‹¤.", None
    
    # subject_idë¡œ í•„í„°ë§
    df_filtered = df_full[df_full['subject_id'] == subject_id]
    
    if df_filtered.empty:
        return "âš ï¸ ê²½ê³ : í•´ë‹¹ í™˜ì(subject_id)ì˜ ê²€ì‚¬ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None
    
    # ì›¹ì— í‘œì‹œí•  ê²€ì‚¬ ê´€ë ¨ ì»¬ëŸ¼ë§Œ ì„ íƒ
    # ìˆ˜ì •: 'hadm_id'ì™€ 'days_of_visit'ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.
    lab_cols = [
        'charttime', 'itemid', 'valuenum', 'valueuom', 'value'
    ]
    # ì¤‘ë³µëœ ê²€ì‚¬ ê²°ê³¼ë¥¼ ì œê±°í•˜ê³ , ìµœê·¼ ê¸°ë¡ ìˆœìœ¼ë¡œ ì •ë ¬ (hadm_idì™€ charttime ê¸°ì¤€)
    # ì •ë ¬ ê¸°ì¤€ì€ 'charttime'ë§Œ ë‚¨ê²¨ë„ ë˜ì§€ë§Œ, í•„í„°ë§ ë¡œì§ ìœ ì§€ë¥¼ ìœ„í•´ hadm_idë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ ê¸°ì¡´ ë¡œì§ì„ ìµœì†Œí•œìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    df_tests = df_filtered[lab_cols + ['hadm_id']].drop_duplicates(
        subset=['hadm_id', 'charttime', 'itemid', 'valuenum'], keep='first'
    ).sort_values(by=['hadm_id', 'charttime'], ascending=False)
    
    # ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•  DataFrameì—ì„œëŠ” 'hadm_id'ë¥¼ ì œì™¸í•©ë‹ˆë‹¤.
    df_tests = df_tests[lab_cols]
    
    info = f"âœ… subject_id={subject_id}ì˜ ì´ {len(df_tests)}ê±´ì˜ ê²€ì‚¬ ê¸°ë¡ ì¡°íšŒ ì™„ë£Œ."
    return info, df_tests

# =========================
# 1) í™˜ì ì¡°íšŒ (JSON ë°˜í™˜ì„ ìœ„í•´ ì»¬ëŸ¼ëª… ìˆ˜ì •)
# =========================
def lookup_patient(subject_id_str: str, patients_csv_path: str):
    if not subject_id_str or not subject_id_str.strip().isdigit():
        return "âŒ subject_idëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.", None
    sid = int(subject_id_str.strip())

    # ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (FastAPIëŠ” ì‹¤í–‰ ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    if not os.path.isabs(patients_csv_path):
         patients_csv_path = os.path.join(BASE_DIR, patients_csv_path)

    if not os.path.isfile(patients_csv_path):
        return f"âŒ patients.csv ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {patients_csv_path}", None

    df = pd.read_csv(patients_csv_path, low_memory=False)

    c_subj   = _pick_col(df, ["subject_id"])
    c_name   = _pick_col(df, ["create", "name", "patient_name"])  # 'create'ê°€ ê¸°ë³¸, ì—†ìœ¼ë©´ fallback
    c_gender = _pick_col(df, ["gender"])
    c_age    = _pick_col(df, ["anchor_age", "age"])
    c_year   = _pick_col(df, ["anchor_year", "birth_year"])

    for req, cname in {
        "subject_id": c_subj, "gender": c_gender, "anchor_age": c_age, "anchor_year": c_year
    }.items():
        if cname is None:
            return f"âŒ patients.csvì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {req}", None
    if c_name is None:
        # ì´ë¦„ ì»¬ëŸ¼ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
        df["__name__"] = "ê¹€ì§€í—Œ"
        c_name = "__name__"

    # ìˆ«ì ë³€í™˜ í›„ í•„í„°
    df[c_subj] = pd.to_numeric(df[c_subj], errors="coerce").astype("Int64")
    sel = df.loc[df[c_subj] == sid, [c_subj, c_name, c_gender, c_age, c_year]].copy()

    if sel.empty:
        return f"âš ï¸ subject_id={sid} ì— í•´ë‹¹í•˜ëŠ” í™˜ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", None

    sel = sel.drop_duplicates().reset_index(drop=True)
    
    # ğŸ’¥ ì¤‘ìš”: Gradioì™€ ë‹¬ë¦¬ JSONì€ ì˜ë¬¸ keyë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    sel.columns = ["subject_id", "patient_name", "gender", "anchor_age", "anchor_year"]

    info = f"âœ… í™˜ì ì¡°íšŒ ì™„ë£Œ: subject_id={sid} (í–‰ {len(sel)}ê°œ)"
    return info, sel


# =========================
# 2) í‡´ì›ì¼ ì˜ˆì¸¡
# =========================
@torch.no_grad()
def run_inference(
    subject_id_str: str,
    unified_csv: str,
    ckpt_path: str,
    use_example_sources: bool,
    sources_json_path: str,
    cache_dir: str,
    encode_device_str: str,  # "cpu" | "cuda" | "mps"
    batch_size: int,
):
    # ì…ë ¥ ê²€ì¦
    if not subject_id_str or not subject_id_str.strip().isdigit():
        return "âŒ subject_idëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.", None
    subject_id = int(subject_id_str.strip())
    
    # ê²½ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not os.path.isabs(unified_csv):
        unified_csv = os.path.join(BASE_DIR, unified_csv)
    if not os.path.isabs(ckpt_path):
        ckpt_path = os.path.join(BASE_DIR, ckpt_path)
    if cache_dir and not os.path.isabs(cache_dir):
        cache_dir = os.path.join(BASE_DIR, cache_dir)

    if not os.path.isfile(unified_csv):
        return f"âŒ unified_csv ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {unified_csv}", None
    if not os.path.isfile(ckpt_path):
        return f"âŒ ì²´í¬í¬ì¸íŠ¸(.pt) ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ckpt_path}", None

    # sources êµ¬ì„±
    if use_example_sources:
        sources = example_sources_config_v3()
    else:
        if not sources_json_path or not os.path.isfile(sources_json_path):
            return "âŒ sources_json_pathê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.", None
        with open(sources_json_path, "r", encoding="utf-8") as f:
            sources = json.load(f)

    # (ì„ íƒ) BRITS CKPT hidden_dim ë¯¸ìŠ¤ë§¤ì¹˜ ë³´ì •(í•„ìš”ì‹œ)
    if "complete_blood_count" in sources:
        sources["complete_blood_count"]["hidden_dim"] = sources["complete_blood_count"].get("hidden_dim", 64)

    # ë‹¨ì¼ DS ìƒì„±(ì „ì²´) â†’ Subsetìœ¼ë¡œ subjectë§Œ ì¶”ì¶œ
    ds_all = HadmTableDatasetV3(
        unified_csv=unified_csv,
        drop_index_death=False,
        drop_30d_postdischarge_death=False,
        crrt_label_csv=None,
        mimic_derived_lods_csv=None,
        mimic_derived_crrt_csv=None,
        crrt_restrict_within_admission_window=True,
        sources=sources,
        encode_device=encode_device_str if encode_device_str in ("cpu","cuda","mps") else "cpu",
        cache_dir=cache_dir if (cache_dir and len(cache_dir.strip())>0) else None,
    )

    idxs = ds_all.df.index[ds_all.df["subject_id"] == subject_id].tolist()
    if len(idxs) == 0:
        return f"âš ï¸ subject_id={subject_id} ì— í•´ë‹¹í•˜ëŠ” HADMì´ ì—†ìŠµë‹ˆë‹¤.", None

    subset = Subset(ds_all, idxs)

    # hadm â†’ (admit, disch)
    tmp = ds_all.df.loc[idxs, ["hadm_id", "admittime", "dischtime"]].copy()
    tmp["admittime"] = pd.to_datetime(tmp["admittime"], errors="coerce")
    tmp["dischtime"] = pd.to_datetime(tmp["dischtime"], errors="coerce")
    times_map = {int(r.hadm_id): (r.admittime, r.dischtime) for r in tmp.itertuples(index=False)}

    # Loader
    loader = DataLoader(
        subset,
        batch_size=max(1, int(batch_size)),
        shuffle=False,
        num_workers=0,
        pin_memory=False,
        collate_fn=collate_hadm_batch_v3,
        drop_last=False,
    )

    # peek â†’ (S,N,E)
    peek = next(iter(loader))
    S = peek["base"].shape[1]
    N = peek["exam_z"].shape[1]
    E = peek["exam_z"].shape[2] if N > 0 else 0

    # ëª¨ë¸ ë¡œë“œ
    device = resolve_device(None)
    model = TableTransformerPredictor(
        num_tables=N, latent_dim=E, base_dim=S,
        d_model=256, nhead=8, depth=3, dim_ff=768,
        dropout=0.15, head_hidden=256,
        use_film=True, use_masked_mean=True
    ).to(device).eval()

    ck = torch.load(ckpt_path, map_location="cpu")
    state_dict = ck.get("model", ck)  # {"model": ...} ë˜ëŠ” state_dict
    model.load_state_dict(state_dict, strict=False)

    # ì¶”ë¡ 
    rows, seen = [], set()
    for batch in loader:
        base = batch["base"].to(device)
        exam_z = batch["exam_z"].to(device)
        exam_mask = batch["exam_mask"].to(device)

        los_pred, readmit_logit = model(base=base, exam_z=exam_z, exam_mask=exam_mask)
        # readmit_prob = torch.sigmoid(readmit_logit)  # í•„ìš”ì‹œ ì‚¬ìš©

        hadm_list = batch["hadm_id"]
        subj_list = batch["subject_id"]
        for i in range(len(hadm_list)):
            hadm = int(hadm_list[i])
            subj = int(subj_list[i])

            lp = float(los_pred[i].item())
            lp_3 = round(lp, 3)  # ì†Œìˆ˜ì  3ìë¦¬

            # ì‹¤ì œ ì…/í‡´ì›ì¼
            admit_dt, disch_true_dt = times_map.get(hadm, (None, None))

            # ì˜ˆì¸¡ í‡´ì›ì¼(LOS 3ìë¦¬ ì‚¬ìš©)
            pred_disch_dt = None
            if admit_dt is not None and pd.notna(admit_dt):
                try:
                    pred_disch_dt = admit_dt + pd.to_timedelta(lp_3, unit="D")
                except Exception:
                    pred_disch_dt = None
            
            # ì˜¤ì°¨(ì¼) â†’ ì†Œìˆ˜ì  3ìë¦¬
            err_days_3 = None
            if (pred_disch_dt is not None) and (disch_true_dt is not None) and pd.notna(disch_true_dt):
                try:
                    err_days = (pred_disch_dt - disch_true_dt).total_seconds() / (24 * 3600.0)
                    err_days_3 = round(err_days, 3)
                except Exception:
                    err_days_3 = None

            key = (subj, hadm)
            if key in seen:
                continue
            seen.add(key)

            rows.append({
                "subject_id": subj,
                "hadm_id": hadm,
                "admittime": admit_dt,
                "dischtime_true": disch_true_dt,
                "pred_dischtime": pred_disch_dt,  # â† lp_3ë¡œ ê³„ì‚°ëœ datetime
                "los_pred_days": lp_3,            # â† ì†Œìˆ˜ì  3ìë¦¬
                "error_days": (float(err_days_3) if err_days_3 is not None else None),  # â† ì†Œìˆ˜ì  3ìë¦¬
            })

    df = pd.DataFrame(
        rows,
        columns=[
            "subject_id", "hadm_id", "admittime", "dischtime_true",
            "pred_dischtime", "los_pred_days", "error_days"
        ],
    ).drop_duplicates().reset_index(drop=True)

    # ë‚ ì§œ ë³´ê¸° ì¢‹ê²Œ
    for c in ["admittime", "dischtime_true", "pred_dischtime"]:
        if c in df.columns:
            # JSON ë°˜í™˜ì„ ìœ„í•´ NaT (Not a Time) ê°’ì„ Noneìœ¼ë¡œ ë³€ê²½
            df[c] = pd.to_datetime(df[c], errors='coerce')
            df[c] = df[c].dt.strftime("%Y-%m-%d %H:%M:%S").replace({pd.NaT: None})


    # ìˆ«ì 3ìë¦¬ ë°˜ì˜¬ë¦¼ (í‘œì‹œëŠ” floatë¡œ ìœ ì§€)
    for c in ["los_pred_days", "error_days"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").round(3)

    info = (
        f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: subject_id={subject_id}, HADM {len(df)}ê±´\n"
        f" - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: {os.path.basename(ckpt_path)}\n"
        f" - í…Œì´ë¸” ìˆ˜ N={N}, ì ì¬ ì°¨ì› E={E}, base ì°¨ì› S={S}"
    )
    return info, df


# subject_idë§Œ ë°›ì•„ ì˜ˆì¸¡ ì‹¤í–‰ (ê³ ì • ì¸ì ë˜í•‘)
def run_inference_defaults(subject_id_str: str):
    # ìˆ˜ì •ëœ ì ˆëŒ€ ê²½ë¡œ ë³€ìˆ˜(UNIFIED_CSV, CKPT_PATH ë“±)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    return run_inference(
        subject_id_str=subject_id_str,
        unified_csv=UNIFIED_CSV,
        ckpt_path=CKPT_PATH,
        use_example_sources=USE_EXAMPLE_SOURCES,
        sources_json_path=SOURCES_JSON_PATH,
        cache_dir=CACHE_DIR,
        encode_device_str=ENCODE_DEVICE,
        batch_size=BATCH_SIZE,
    )


# --- 4. FastAPI ì•± ì„¤ì • ---
app = FastAPI(title="Patient Prediction API")

# (ì„ íƒì‚¬í•­) CORS ì„¤ì •: í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ í¬íŠ¸ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 5. API ì—”ë“œí¬ì¸íŠ¸(URL) ì •ì˜ ---

@app.get("/api/patient/{subject_id}")
async def get_patient_info(subject_id: str):
    """
    í™˜ì IDë¥¼ ë°›ì•„ í™˜ì ê¸°ë³¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. lookup_patient í•¨ìˆ˜ ì‹¤í–‰ (ì ˆëŒ€ ê²½ë¡œ PATIENTS_CSV ì‚¬ìš©)
    info, sel = lookup_patient(subject_id, PATIENTS_CSV) 
    
    # 2. ì˜¤ë¥˜ ì²˜ë¦¬ (selì´ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆì„ ë•Œ)
    if sel is None or sel.empty:
        # info ë³€ìˆ˜ì— ë‹´ê¸´ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ 404/500 ì—ëŸ¬ë¡œ ë°˜í™˜
        error_detail = info.replace("âŒ ", "").replace("âš ï¸ ", "")
        if "ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" in error_detail or "ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤" in error_detail:
             raise HTTPException(status_code=500, detail=error_detail) # ì„œë²„ ì„¤ì • ì˜¤ë¥˜
        else:
             raise HTTPException(status_code=404, detail=error_detail) # ë°ì´í„° ì—†ìŒ
    
    # 3. ì„±ê³µ ì‹œ: DataFrameì˜ ì²« ë²ˆì§¸ í–‰ì„ JSON(dict)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    # .iloc[0].to_dict()ëŠ” NaT/NaN ê°’ì„ JSONì´ ì²˜ë¦¬ ëª»í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
    # Pandasì˜ JSON ë³€í™˜ ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    result_json = json.loads(sel.iloc[[0]].to_json(orient="records"))[0]
    return result_json


@app.get("/api/predict/{subject_id}")
async def get_prediction(subject_id: str):
    """
    í™˜ì IDë¥¼ ë°›ì•„ í‡´ì›ì¼ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. run_inference_defaults í•¨ìˆ˜ ì‹¤í–‰
    info, df_result = run_inference_defaults(subject_id)
    
    # 2. ì˜¤ë¥˜ ì²˜ë¦¬
    if df_result is None:
        error_detail = info.replace("âŒ ", "").replace("âš ï¸ ", "")
        if "ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" in error_detail:
            raise HTTPException(status_code=500, detail=error_detail) # ì„œë²„ ì„¤ì • ì˜¤ë¥˜
        else:
             raise HTTPException(status_code=404, detail=error_detail) # ë°ì´í„° ì—†ìŒ
    
    # 3. ì„±ê³µ ì‹œ: ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ DataFrameì„ JSON (List[dict])ìœ¼ë¡œ ë°˜í™˜
    return {
        "status_message": "âœ… ì˜ˆì¸¡ì™„ë£Œ", #info ìƒì„¸ì •ë³´ ì‚¬ìš©ì‹œ info
        "predictions": json.loads(df_result.to_json(orient="records"))
    }

# ----------------------------------------------------------------------------
# 5. API ì—”ë“œí¬ì¸íŠ¸(URL) ì •ì˜: ìƒˆ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
# ----------------------------------------------------------------------------

@app.get("/api/tests/{subject_id}")
async def get_lab_tests(subject_id: str):
    """
    í™˜ì IDë¥¼ ë°›ì•„ í•´ë‹¹ í™˜ìì˜ ì„ìƒ ê²€ì‚¬ ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    info, df_result = lookup_lab_tests(subject_id, UNIFIED_CSV)
    
    if df_result is None:
        error_detail = info.replace("âŒ ", "").replace("âš ï¸ ", "")
        status_code = 500 if "ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" in error_detail or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in error_detail else 404
        raise HTTPException(status_code=status_code, detail=error_detail)
    
    # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "status_message": info,
        "lab_tests": json.loads(df_result.to_json(orient="records")),
        "item_id_map": ITEMID_MAPPING # ë§¤í•‘ ì •ë³´ ì¶”ê°€
    }

# --- 6. í”„ë¡ íŠ¸ì—”ë“œ íŒŒì¼ ì„œë¹™ (404 ì˜¤ë¥˜ í•´ê²°) ---
FRONTEND_DIR = os.path.join(BASE_DIR, "../../frontend") # sw/frontend ê²½ë¡œ

# 1) ë£¨íŠ¸ ê²½ë¡œ '/' ìš”ì²­ ì‹œ index.htmlì„ ì„œë¹™í•©ë‹ˆë‹¤.
@app.get("/", response_class=HTMLResponse)
async def serve_index():
    index_path = os.path.join(FRONTEND_DIR, "index.html")
    if not os.path.exists(index_path):
        return HTMLResponse("<h1>Error: index.html not found. Check if the 'frontend' folder is at the correct path.</h1>", status_code=500)
    with open(index_path, 'r', encoding='utf-8') as f:
        return f.read()

# 2) ì •ì  íŒŒì¼ (CSS, JS) ìš”ì²­ ì‹œ '/static' ê²½ë¡œë¡œ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.
# ì´ì œ index.htmlì—ì„œ <link rel="stylesheet" href="/static/style.css"> ë¡œ íŒŒì¼ì„ ì œëŒ€ë¡œ ì°¾ìŠµë‹ˆë‹¤.
app.mount(
    "/static", 
    StaticFiles(directory=FRONTEND_DIR), 
    name="static"
)

# --- 7. ì„œë²„ ì‹¤í–‰ (uvicorn) ---
if __name__ == "__main__":
    import uvicorn
    # uvicorn main:app --reload --host 127.0.0.1 --port 8000
    print(f"FastAPI ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (BASE_DIR: {BASE_DIR})")
    print(f"Frontend ê²½ë¡œ: {FRONTEND_DIR}")
    print("í„°ë¯¸ë„ì—ì„œ 'uvicorn main:app --reload --host 127.0.0.1 --port 8000' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    uvicorn.run(app, host="127.0.0.1", port=8000)